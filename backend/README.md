# Kaso AI Assistant - Backend

FastAPI backend with RAG (Retrieval Augmented Generation) for intelligent Q&A.

## Features

- ðŸš€ **FastAPI** - High-performance async API
- âš¡ **uvloop** - Enhanced async performance (Linux/Mac)
- ðŸ”’ **API Key Auth** - Secure endpoints with X-API-Key header
- ðŸ“¡ **SSE Streaming** - Real-time response streaming
- ðŸ§  **RAG Pipeline** - Retrieve, Rerank, Generate
- ðŸŒ **Multilingual** - Arabic and English support
- ðŸ’¾ **SQLite** - Async conversation storage
- ðŸ” **Vector Search** - ChromaDB for semantic search

## Requirements

- Python 3.10+
- Groq API Key ([Get free key](https://console.groq.com/))

## Setup

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
.\venv\Scripts\activate
# Activate (Linux/Mac)
# source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
copy .env.example .env  # Windows
# cp .env.example .env  # Linux/Mac

# Edit .env with your API keys

# Run the server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `GROQ_API_KEY` | Your Groq API key | Required |
| `API_SECRET_KEY` | Secret key for API authentication | Required |
| `CORS_ORIGINS` | Allowed CORS origins | http://localhost:3000 |
| `LLM_MODEL` | Groq model to use | llama-3.1-8b-instant |
| `EMBEDDING_MODEL` | Embedding model | paraphrase-multilingual-MiniLM-L12-v2 |

## API Endpoints

### Chat

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/chat/stream` | Streaming chat (SSE) |
| `POST` | `/api/chat` | Non-streaming chat |

### Conversations

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/conversations` | List all conversations |
| `POST` | `/api/conversations` | Create new conversation |
| `GET` | `/api/conversations/{id}` | Get conversation with messages |
| `PATCH` | `/api/conversations/{id}` | Update conversation title |
| `DELETE` | `/api/conversations/{id}` | Delete conversation |

### Search

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/search/conversations` | Keyword search in history |
| `POST` | `/api/search` | Semantic search |
| `GET` | `/api/search/knowledge` | Search knowledge base |

### Health

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Basic health check |
| `GET` | `/health` | Detailed health status |

> **Note:** All endpoints except `/` and `/health` require `X-API-Key` header.

## Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry
â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”œâ”€â”€ api/                 # API endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py          # Chat endpoints
â”‚   â”‚   â”œâ”€â”€ conversations.py # CRUD endpoints
â”‚   â”‚   â””â”€â”€ search.py        # Search endpoints
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â””â”€â”€ auth.py          # API key middleware
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ database.py      # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic schemas
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ embedding_service.py  # Text embeddings
â”‚       â”œâ”€â”€ chroma_service.py     # Vector DB
â”‚       â”œâ”€â”€ reranker_service.py   # Reranking
â”‚       â”œâ”€â”€ llm_service.py        # Groq API
â”‚       â””â”€â”€ rag_service.py        # RAG pipeline
â”œâ”€â”€ data_pipeline/           # Data processing
â””â”€â”€ data/                    # Data storage
```

## Data Pipeline

See [Data Pipeline Documentation](data_pipeline/README.md) for details on adding new data sources.

```bash
# Run full pipeline
python -m data_pipeline.run_pipeline

# Or run steps individually
python -m data_pipeline.scraper    # Fetch URLs
python -m data_pipeline.cleaner    # Clean content
python -m data_pipeline.chunker    # Split documents
python -m data_pipeline.indexer    # Index to ChromaDB
```
