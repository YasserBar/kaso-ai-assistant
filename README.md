# Kaso AI Assistant ğŸ¤–

AI-powered assistant for Kaso. Ask questions about Kaso and get intelligent answers based on collected knowledge.

> **Prototype Designed by:** Yasser Barghouth
> **Developed with:** Claude Code using Opus 4.5 & Antigravity using Gemini 3 Pro

## Features

- ğŸ§  **RAG-based AI**: Retrieval Augmented Generation for accurate answers
- ğŸŒ **Multilingual**: Supports 100+ languages (Arabic, English, French, Spanish, German, etc.)
  - Multilingual embedding model: `paraphrase-multilingual-MiniLM-L12-v2`
  - Multilingual reranker: `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`
- ğŸŒ **UI Localization**: Full interface support for 3 languages (English, Arabic, German) with RTL support
- âš¡ **Fast Streaming**: Real-time response streaming with Groq LPU
- ğŸ’¬ **Chat History**: Persistent conversation history with search
- ğŸ”’ **Secure**: API key authentication
- ğŸ“± **Responsive**: Works on desktop and mobile
- ğŸ” **Advanced Retrieval**: Vector search + reranking for better accuracy

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend                            â”‚
â”‚                   (Next.js + assistant-ui)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ SSE Streaming
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Backend                            â”‚
â”‚                    (FastAPI + uvloop)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   RAG   â”‚  â”‚  LLM    â”‚  â”‚ History â”‚  â”‚   Reranker      â”‚ â”‚
â”‚  â”‚ Service â”‚  â”‚ Service â”‚  â”‚ Service â”‚  â”‚   Service       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚            â”‚            â”‚                 â”‚         â”‚
â”‚       â–¼            â–¼            â–¼                 â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ChromaDB â”‚  â”‚  Groq   â”‚  â”‚ SQLite  â”‚  â”‚ Cross-Encoder   â”‚ â”‚
â”‚  â”‚(Vectors)â”‚  â”‚   API   â”‚  â”‚   DB    â”‚  â”‚     Model       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- Groq API Key ([Get free key](https://console.groq.com/))

### 1. Clone and Setup Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
.\venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Copy and edit environment file
copy .env.example .env
# Edit .env with your API keys
```

### 2. Run Data Pipeline

**Important**: You must run the data pipeline to build the knowledge base before starting the backend.

```bash
cd backend

# Option 1: Run complete pipeline (Recommended for first time)
# This will: scrape URLs â†’ clean â†’ chunk â†’ index into ChromaDB
python -m data_pipeline.run_pipeline --markdown ../kaso_research_report.md

# Option 2: Run steps individually
# Step 1: Scrape URLs from kaso_data_sources.csv
python -m data_pipeline.scraper

# Step 2: Clean scraped content
python -m data_pipeline.cleaner

# Step 3: Chunk documents
python -m data_pipeline.chunker
python -m data_pipeline.chunker --markdown ../kaso_research_report.md

# Step 4: Index into ChromaDB
python -m data_pipeline.indexer
```

**Note**: The pipeline will:
- Scrape 25+ URLs from `kaso_data_sources.csv` (some may fail due to anti-bot protection)
- Process the main research report `kaso_research_report.md`
- Create embeddings and index ~165 chunks into ChromaDB

### 3. Start Backend Server

```bash
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 4. Setup and Start Frontend

```bash
cd frontend

# Install dependencies
npm install

# Copy and edit environment file
# Create .env.local with:
# NEXT_PUBLIC_API_URL=http://localhost:8000
# NEXT_PUBLIC_API_KEY=your_secret_key

# Start development server
npm run dev
```

### 5. Open in Browser

Visit [http://localhost:3000](http://localhost:3000)

## Project Structure

```
kaso_ai_assistant/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/               # API endpoints
â”‚   â”‚   â”œâ”€â”€ middleware/        # Auth middleware
â”‚   â”‚   â”œâ”€â”€ models/            # Database & Pydantic models
â”‚   â”‚   â””â”€â”€ services/          # Business logic
â”‚   â”œâ”€â”€ data_pipeline/         # Data processing
â”‚   â”‚   â”œâ”€â”€ scraper.py         # URL scraper
â”‚   â”‚   â”œâ”€â”€ cleaner.py         # Text cleaner
â”‚   â”‚   â”œâ”€â”€ chunker.py         # Text splitter
â”‚   â”‚   â””â”€â”€ indexer.py         # ChromaDB indexer
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ kaso_data_sources.csv  # URL sources
â”‚       â””â”€â”€ chroma_db/         # Vector database
â”‚
â”œâ”€â”€ frontend/                   # Next.js Frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ app/               # Next.js app router
â”‚       â”œâ”€â”€ components/        # React components
â”‚       â””â”€â”€ lib/               # Utilities & API client
â”‚
â””â”€â”€ README.md
```

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/chat/stream` | Streaming chat (SSE) |
| POST | `/api/chat` | Non-streaming chat |
| GET | `/api/conversations` | List conversations |
| GET | `/api/conversations/{id}` | Get conversation |
| DELETE | `/api/conversations/{id}` | Delete conversation |
| GET | `/api/search/conversations` | Search history |
| GET | `/api/search/knowledge` | Search knowledge base |

All endpoints require `X-API-Key` header.

## Adding New Data Sources

### Method 1: Add URLs to scrape

1. Add URL to `backend/data/kaso_data_sources.csv`:
```csv
Ø§Ù„Ù…ØµØ§Ø¯Ø±,Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
29,https://new-source.com/article
```

2. Run the scraper and pipeline:
```bash
cd backend
# Scrape new URLs only
python -m data_pipeline.scraper

# Process and index (skip scraping)
python -m data_pipeline.run_pipeline --no-scrape
```

### Method 2: Add markdown files directly

1. Place your markdown file in the project root or `backend/data/`

2. Run the chunker and indexer:
```bash
cd backend
python -m data_pipeline.chunker --markdown path/to/your/file.md
python -m data_pipeline.indexer
```

## Tech Stack

| Component | Technology |
|-----------|------------|
| LLM | Groq (llama-3.1-8b-instant) |
| Embeddings | paraphrase-multilingual-MiniLM-L12-v2 |
| Vector DB | ChromaDB |
| Reranker | cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 (Multilingual) |
| Backend | FastAPI + uvloop |
| Frontend | Next.js + Tailwind CSS |
| Database | SQLite (async) |

## License

MIT
